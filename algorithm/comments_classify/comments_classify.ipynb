{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考https://www.kaggle.com/hawkeoni/pytorch-simple-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertModel, AdamW, WarmupLinearSchedule, BertPreTrainedModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda\n"
     ]
    }
   ],
   "source": [
    "path = \"./input/\"\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('use cuda')\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# tokenizer = BertTokenizer.from_pretrained('./chinese_L-12_H-768_A-12/bert_model.ckpt.index',from_tf=True)\n",
    "assert tokenizer.pad_token_id == 0, \"Padding value used in masks is set to zero, please change it everywhere\"\n",
    "train_df = pd.read_csv(os.path.join(path, 'ai_challenger_sentiment_analysis_trainingset_20180816/sentiment_analysis_trainingset.csv'))\n",
    "val_df = pd.read_csv(os.path.join(path, 'ai_challenger_sentiment_analysis_validationset_20180816/sentiment_analysis_validationset.csv'))\n",
    "# training on a part of data for speed\n",
    "# train_df = train_df.sample(frac=0.33)\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105000it [12:06, 144.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip rows: 0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15000it [05:24, 46.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip rows: 0, 0.0\n"
     ]
    }
   ],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, dataframe, device):\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_idx = tokenizer.pad_token_id\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        self.labels = ['location_traffic_convenience', 'location_distance_from_business_district', 'location_easy_to_find', 'service_wait_time', 'service_waiters_attitude', 'service_parking_convenience', 'service_serving_speed', 'price_level', 'price_cost_effective', 'price_discount', 'environment_decoration', 'environment_noise', 'environment_space', 'environment_cleaness', 'dish_portion', 'dish_taste', 'dish_look', 'dish_recommendation', 'others_overall_experience', 'others_willing_to_consume_again']\n",
    "        max_tokens = 0\n",
    "        for i, (row) in tqdm(dataframe.iterrows()):\n",
    "            tokens = tokenizer.tokenize(row[\"content\"])\n",
    "            sentence = row[\"content\"]\n",
    "            if len(tokens) > 400:\n",
    "                tokens = tokens[0:400]\n",
    "                # max_tokens += 1\n",
    "                # continue\n",
    "                sentence = row[\"content\"][0:400]\n",
    "#             print(sentence)\n",
    "            text = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "            text = torch.LongTensor(text)\n",
    "            # 将label展开成全联合分布,并平均到20类的可信程度为1/20\n",
    "            tags = torch.FloatTensor(1,4*features_size)\n",
    "            start = 0\n",
    "            for c in row[self.labels]:\n",
    "                m = (c==np.array([-2,-1,0,1]))\n",
    "                tags[0,start:(start+4)]=torch.from_numpy(m)\n",
    "                start+=4\n",
    "            self.X.append(text)\n",
    "            self.Y.append(tags/features_size)\n",
    "#         print(f\"skip rows: {max_tokens}, {max_tokens/len(self.X)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "def collate_fn(batch: List[Tuple[torch.LongTensor, torch.LongTensor]]) \\\n",
    "        -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    x, y = list(zip(*batch))\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    y = torch.stack(y)\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "train_dataset = ToxicDataset(tokenizer, train_df, device)\n",
    "dev_dataset = ToxicDataset(tokenizer, val_df, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "dev_sampler = RandomSampler(dev_dataset)\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "dev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE, sampler=dev_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BertClassifier, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, features_size*4)\n",
    "#         self.classifiers = []\n",
    "#         for idx in range(4):\n",
    "#             # index 0 map to emotion -2, 1 to -1, 2 to 0,3 to 1\n",
    "#             self.classifiers.append(nn.Linear(config.hidden_size, 20))\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                \n",
    "            labels=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask)\n",
    "        cls_output = outputs[1] # batch, hidden\n",
    "#         out = torch.zeros(4,20)#.to(device)\n",
    "#         tem = torch.sigmoid(self.classifiers[0](cls_output))\n",
    "#         print(tem.shape)\n",
    "#         for row in range(4):\n",
    "#             print(out[row,:].shape,cls_output.shape)\n",
    "#             temp = self.classifiers[row](cls_output) # batch, 20\n",
    "#             out[row,:] = torch.sigmoid(temp)\n",
    "#         print('1',cls_output.shape)\n",
    "        cls_output = self.classifier(cls_output)\n",
    "        cls_output = torch.sigmoid(cls_output)\n",
    "#         print('2',cls_output.shape)\n",
    "#         print('label',labels.shape)\n",
    "        criterion = nn.BCELoss()\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = criterion(cls_output.flatten(), labels)\n",
    "        return loss, cls_output\n",
    "\n",
    "model = BertClassifier.from_pretrained('bert-base-chinese').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        mask = (x != 0).float()\n",
    "        y = y.flatten()\n",
    "        loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Train loss {total_loss / len(iterator)}\")\n",
    "\n",
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(iterator):\n",
    "            mask = (x != 0).float()\n",
    "            y = y.flatten()\n",
    "            loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "#             print('output: ',outputs.shape,' y:',y.shape)\n",
    "            total_loss += loss\n",
    "            true += y.cpu().numpy().tolist()\n",
    "            pred += outputs.cpu().numpy().tolist()\n",
    "    true = np.argmax(np.array(true).reshape((-1,features_size,4)),axis=2)\n",
    "    pred = np.argmax(np.array(pred).reshape((-1,features_size,4)),axis=2)\n",
    "\n",
    "    total_f1 = 0\n",
    "    for i, name in enumerate(['location_traffic_convenience', 'location_distance_from_business_district', 'location_easy_to_find', 'service_wait_time', 'service_waiters_attitude', 'service_parking_convenience', 'service_serving_speed', 'price_level', 'price_cost_effective', 'price_discount', 'environment_decoration', 'environment_noise', 'environment_space', 'environment_cleaness', 'dish_portion', 'dish_taste', 'dish_look', 'dish_recommendation', 'others_overall_experience', 'others_willing_to_consume_again']):\n",
    "        f1_value = f1_score(true[:, i], pred[:, i],labels=[0,1,2,3],average ='micro')\n",
    "        total_f1 += f1_value\n",
    "        print(f\"{name} f1 {f1_value}\")\n",
    "    print(f\"Evaluate loss {total_loss / len(iterator)}\")\n",
    "    print(f\"average f1: {total_f1/features_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "EPOCH_NUM = 2\n",
    "# triangular learning rate, linearly grows untill half of first epoch, then linearly decays \n",
    "warmup_steps = int(0.5 * len(train_iterator))\n",
    "total_steps = len(train_iterator) * EPOCH_NUM - warmup_steps\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== EPOCH 0 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 26250/26250 [2:27:06<00:00,  2.97it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.08241832897095454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [05:29<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_traffic_convenience f1 0.9438\n",
      "location_distance_from_business_district f1 0.8927333333333334\n",
      "location_easy_to_find f1 0.9162666666666667\n",
      "service_wait_time f1 0.8932666666666667\n",
      "service_waiters_attitude f1 0.8229333333333333\n",
      "service_parking_convenience f1 0.9654666666666667\n",
      "service_serving_speed f1 0.9238666666666666\n",
      "price_level f1 0.7799333333333334\n",
      "price_cost_effective f1 0.8630666666666666\n",
      "price_discount f1 0.8288666666666666\n",
      "environment_decoration f1 0.8518000000000001\n",
      "environment_noise f1 0.8474666666666666\n",
      "environment_space f1 0.8168666666666666\n",
      "environment_cleaness f1 0.8624666666666667\n",
      "dish_portion f1 0.7648\n",
      "dish_taste f1 0.7498\n",
      "dish_look f1 0.779\n",
      "dish_recommendation f1 0.8825333333333333\n",
      "others_overall_experience f1 0.8000000000000002\n",
      "others_willing_to_consume_again f1 0.823\n",
      "Evaluate loss 0.05534471571445465\n",
      "average f1: 0.8503966666666667\n",
      "================================================== EPOCH 1 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 26250/26250 [2:26:52<00:00,  2.98it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.05504928433880919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 3750/3750 [05:30<00:00, 11.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_traffic_convenience f1 0.9428\n",
      "location_distance_from_business_district f1 0.8891333333333333\n",
      "location_easy_to_find f1 0.9188666666666667\n",
      "service_wait_time f1 0.8971333333333333\n",
      "service_waiters_attitude f1 0.8276666666666667\n",
      "service_parking_convenience f1 0.9667333333333333\n",
      "service_serving_speed f1 0.9292\n",
      "price_level f1 0.7906\n",
      "price_cost_effective f1 0.8727999999999999\n",
      "price_discount f1 0.8342\n",
      "environment_decoration f1 0.8584\n",
      "environment_noise f1 0.8635333333333334\n",
      "environment_space f1 0.8333333333333334\n",
      "environment_cleaness f1 0.8661333333333333\n",
      "dish_portion f1 0.7802666666666667\n",
      "dish_taste f1 0.7655333333333333\n",
      "dish_look f1 0.7933333333333333\n",
      "dish_recommendation f1 0.8871333333333333\n",
      "others_overall_experience f1 0.8092666666666667\n",
      "others_willing_to_consume_again f1 0.8293333333333334\n",
      "Evaluate loss 0.055033281445503235\n",
      "average f1: 0.85777\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCH_NUM):\n",
    "    print('=' * 50, f\"EPOCH {i}\", '=' * 50)\n",
    "    train(model, train_iterator, optimizer, scheduler)\n",
    "    evaluate(model, dev_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model\\\\vocab.txt',\n",
       " './model\\\\special_tokens_map.json',\n",
       " './model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir='./model'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# state = {'net':model.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':2}\n",
    "# torch.save(state,output_dir+'/torch_point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15000it [00:59, 250.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip rows: 0, 0.0\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "test_df = pd.read_csv(os.path.join(path, 'ai_challenger_sentiment_analysis_testa_20180816/sentiment_analysis_testa.csv'))\n",
    "test_dataset = ToxicDataset(test_tokenizer, test_df, device)\n",
    "\n",
    "test_sampler = RandomSampler(test_dataset)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|▋| 2792/3750 [04:05<01:24, 11.34it/s]"
     ]
    }
   ],
   "source": [
    "test_model = BertClassifier.from_pretrained(output_dir).to(device)\n",
    "evaluate(test_model,test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
